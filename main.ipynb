{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T18:31:00.841218Z",
     "start_time": "2020-04-07T18:31:00.813814Z"
    }
   },
   "outputs": [],
   "source": [
    "# Modules used for PySpark solution\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer, StopWordsRemover, StringIndexer\n",
    "from pyspark.ml import Pipeline as PySparkPipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.clustering import KMeans as PySparkKMeans\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Modules used for non distributed solution\n",
    "import collections\n",
    "import spacy\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "def create_session():\n",
    "    ''' Function used to instantiate a pySpark Session with \n",
    "    the specific configurations'''\n",
    "    sc_conf = SparkConf()\n",
    "    sc_conf.setAppName('SparkPreProcessing')\n",
    "    sc_conf.setMaster('local')\n",
    "    sc_conf.set('spark.executor.memory', '6g')\n",
    "    sc_conf.set('spark.executor.cores', '8')\n",
    "    sc_conf.set('spark.logConf', True)\n",
    "    print(sc_conf.getAll())\n",
    "    sc = SparkContext.getOrCreate(conf=sc_conf)\n",
    "    ss = SparkSession(sc)\n",
    "    return ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PySpark\n",
    "\n",
    "## 1.1 Loading Files and Creating Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T18:31:05.045245Z",
     "start_time": "2020-04-07T18:31:04.908442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.executor.memory', '6g'), ('spark.master', 'local'), ('spark.logConf', 'True'), ('spark.submit.deployMode', 'client'), ('spark.executor.cores', '8'), ('spark.ui.showConsoleProgress', 'true'), ('spark.app.name', 'SparkPreProcessing')]\n",
      "CPU times: user 15.7 ms, sys: 4.58 ms, total: 20.3 ms\n",
      "Wall time: 126 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Parquet files are self-describing so the schema is preserved.\n",
    "# The result of loading a parquet file is also a DataFrame.\n",
    "RELATIVE_FOLDER_PATH = \"assets/data/\"\n",
    "filename = \"data\"\n",
    "pyspark_session = create_session()\n",
    "\n",
    "ailab_df = pyspark_session.read.parquet(RELATIVE_FOLDER_PATH +\"/data.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Preprocessing and Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T18:31:13.571440Z",
     "start_time": "2020-04-07T18:31:05.048173Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 91.4 ms, sys: 17.1 ms, total: 108 ms\n",
      "Wall time: 8.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ailab_df.cache().count()\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"stopWordsRemovedTokens\")\n",
    "hashingTF = HashingTF(inputCol=\"stopWordsRemovedTokens\", outputCol=\"rawFeatures\", numFeatures=2000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5)\n",
    "label_stringIdx = StringIndexer(inputCol = \"process_class\", outputCol = \"label\")\n",
    "\n",
    "pre_processing_pipeline = PySparkPipeline(stages=[tokenizer, remover, hashingTF, idf, label_stringIdx])\n",
    "\n",
    "pre_processing_pipeline_model = pre_processing_pipeline.fit(ailab_df)\n",
    "\n",
    "treated_df = pre_processing_pipeline_model.transform(ailab_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T18:32:25.692974Z",
     "start_time": "2020-04-07T18:32:25.571937Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PySparkKMeans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PySparkKMeans' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kmeans = PySparkKMeans(k=20)\n",
    "kmeans_trained_model = kmeans.fit(treated_df)\n",
    "kmeans_result_df = kmeans_trained_model.transform(treated_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Classifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T18:31:42.706074Z",
     "start_time": "2020-04-07T18:31:13.603418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "|                          text|process_class|                   probability|label|prediction|\n",
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "| i\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "  ...|          ARE|[0.9999932589578077,3.97377...|  0.0|       0.0|\n",
      "| SEÇÃO V\n",
      "Da Execução\n",
      "\n",
      "Art. ...|          ARE|[0.9992210908220771,4.54681...|  0.0|       0.0|\n",
      "| Rodrigues dos Santos & Sou...|           RE|[0.9904324939584316,0.00632...|  1.0|       0.0|\n",
      "| IXA\n",
      "\n",
      "privada, não é razoáv...|          ARE|[0.9887038042609323,0.01085...|  0.0|       0.0|\n",
      "| JUNTADA\n",
      "junto aos presente...|          ARE|[0.9851287444111012,0.01310...|  0.0|       0.0|\n",
      "| mxl o\n",
      "\n",
      "ADVOC\n",
      "\n",
      "MARCATTO\n",
      "\n",
      "se...|          ARE|[0.9836183978290796,0.01266...|  0.0|       0.0|\n",
      "| ESTADO DE SANTA CATARINA J...|          ARE|[0.9818315239268227,0.01594...|  0.0|       0.0|\n",
      "| fis. 155\n",
      "\n",
      "ESTADO DE SANTA ...|          ARE|[0.9800130632420322,0.01802...|  0.0|       0.0|\n",
      "| fis. 151\n",
      "\n",
      "ESTADO DE SANTA ...|          ARE|[0.9785899451018625,0.01930...|  0.0|       0.0|\n",
      "| ESTADO DE SANTA CATARINA J...|          ARE|[0.9767300696452298,0.02106...|  0.0|       0.0|\n",
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 28 ms, sys: 12.1 ms, total: 40.2 ms\n",
      "Wall time: 29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "(trainingData, testData) = treated_df.randomSplit([0.7, 0.3], seed = 100)\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions = lrModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"text\",\"process_class\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T18:32:21.430184Z",
     "start_time": "2020-04-07T18:31:42.707803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.77 ms, sys: 0 ns, total: 9.77 ms\n",
      "Wall time: 38.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7279028242295433"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T18:32:21.437930Z",
     "start_time": "2020-04-07T18:32:21.432067Z"
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Spacy e Sci-kit learn\n",
    "\n",
    "## 2.1 Loading files and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T18:32:24.777660Z",
     "start_time": "2020-04-07T18:32:21.441159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2036 entries, 0 to 2035\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   process_class  2036 non-null   object\n",
      " 1   process_id     2036 non-null   object\n",
      " 2   doc_id         2036 non-null   object\n",
      " 3   path_img       2036 non-null   object\n",
      " 4   text           2036 non-null   object\n",
      " 5   doc_type       2036 non-null   object\n",
      " 6   num_pag        2036 non-null   int64 \n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 111.5+ KB\n",
      "None\n",
      "CPU times: user 2.86 s, sys: 322 ms, total: 3.18 s\n",
      "Wall time: 3.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "VECTOR_MODEL_NAME = \"pt_core_news_sm\"\n",
    "NLP_SPACY = spacy.load(VECTOR_MODEL_NAME)\n",
    "RELATIVE_FOLDER_PATH = \"assets/data/\"\n",
    "filename = \"data\"\n",
    "stopwords_set = set(STOP_WORDS)\n",
    "\n",
    "parquet_filename = RELATIVE_FOLDER_PATH + filename + \".parquet.gzip\"\n",
    "ailab_df = pd.read_parquet(parquet_filename)\n",
    "print(ailab_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Preprocessing and Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T18:32:25.022441Z",
     "start_time": "2020-04-07T18:32:24.780731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 232 ms, sys: 3.62 ms, total: 236 ms\n",
      "Wall time: 236 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = NLP_SPACY.Defaults.create_tokenizer(NLP_SPACY)\n",
    "raw_text = ailab_df['text'].to_list()\n",
    "\n",
    "tokenized_text = []\n",
    "for row in raw_text[:20]:\n",
    "    doc = tokenizer(row)\n",
    "    preprocessed_doc = [token for token in doc if not token.norm_ in stopwords_set]\n",
    "    tokenized_text.append(\" \".join([word.text for word in preprocessed_doc]))\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "''' Encapsuling components in pipeline '''\n",
    "pipeline = Pipeline([\n",
    "    ('count_vectorizer', count_vectorizer),\n",
    "    ('tfidf_transformer', tfidf_transformer)\n",
    "])\n",
    "\n",
    "vectorized_docs = pipeline.fit_transform(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T18:32:25.429972Z",
     "start_time": "2020-04-07T18:32:25.024152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 815 ms, sys: 468 ms, total: 1.28 s\n",
      "Wall time: 402 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kmeans = KMeans(20)\n",
    "kmeans.fit(vectorized_docs)\n",
    "clustering = collections.defaultdict(list)\n",
    "\n",
    "for idx, label in enumerate(kmeans.labels_):\n",
    "    clustering[label].append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Classyfing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T18:32:25.442165Z",
     "start_time": "2020-04-07T18:32:25.432464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.96 ms, sys: 0 ns, total: 5.96 ms\n",
      "Wall time: 2.92 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "targets_labels = ailab_df['process_class'].to_list()[:20]\n",
    "''' Let's evaluate more deeply the best model '''\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     vectorized_docs,\n",
    "    targets_labels,\n",
    "    test_size=0.25, random_state=42)\n",
    "\n",
    "clf = SGDClassifier()\n",
    "\n",
    "train1 = X_train\n",
    "labelsTrain1 = y_train\n",
    "test1 = X_test\n",
    "labelsTest1 = y_test\n",
    "\"\"\"  train \"\"\"\n",
    "clf.fit(train1, labelsTrain1)\n",
    "\"\"\"  test \"\"\"\n",
    "preds = clf.predict(test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T18:32:25.569825Z",
     "start_time": "2020-04-07T18:32:25.444849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 2, does not match size of target_names, 3. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/PythonEnviroments/ailab-researching/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   1993\u001b[0m                 \u001b[0;34m\"Number of classes, {0}, does not match size of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m                 \u001b[0;34m\"target_names, {1}. Try specifying the labels \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1995\u001b[0;31m                 \u001b[0;34m\"parameter\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1996\u001b[0m             )\n\u001b[1;32m   1997\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of classes, 2, does not match size of target_names, 3. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"accuracy:\", accuracy_score(labelsTest1, preds))\n",
    "print(\n",
    "    classification_report(\n",
    "        labelsTest1,\n",
    "        preds,\n",
    "        target_names=ailab_df['process_class'].unique()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
