{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T18:45:18.287775Z",
     "start_time": "2020-04-07T18:45:18.264057Z"
    }
   },
   "outputs": [],
   "source": [
    "# Modules used for PySpark solution\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer, StopWordsRemover, StringIndexer\n",
    "from pyspark.ml import Pipeline as PySparkPipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.clustering import KMeans as PySparkKMeans\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Modules used for non distributed solution\n",
    "import collections\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.cluster import KMeans\n",
    "from fastparquet import write \n",
    "import pandas as pd\n",
    "\n",
    "def create_session():\n",
    "    ''' Function used to instantiate a pySpark Session with \n",
    "    the specific configurations'''\n",
    "    sc_conf = SparkConf()\n",
    "    sc_conf.setAppName('SparkPreProcessing')\n",
    "    sc_conf.setMaster('local')\n",
    "    sc_conf.set('spark.executor.memory', '6g')\n",
    "    sc_conf.set('spark.executor.cores', '8')\n",
    "    sc_conf.set('spark.logConf', True)\n",
    "    print(sc_conf.getAll())\n",
    "    sc = SparkContext.getOrCreate(conf=sc_conf)\n",
    "    ss = SparkSession(sc)\n",
    "    return ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PySpark\n",
    "\n",
    "## 1.1 Loading Files and Creating Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T19:02:01.213988Z",
     "start_time": "2020-04-07T19:02:01.106690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.executor.memory', '6g'), ('spark.master', 'local'), ('spark.logConf', 'True'), ('spark.submit.deployMode', 'client'), ('spark.executor.cores', '8'), ('spark.ui.showConsoleProgress', 'true'), ('spark.app.name', 'SparkPreProcessing')]\n",
      "CPU times: user 6.61 ms, sys: 4.44 ms, total: 11.1 ms\n",
      "Wall time: 94.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Parquet files are self-describing so the schema is preserved.\n",
    "# The result of loading a parquet file is also a DataFrame.\n",
    "RELATIVE_FOLDER_PATH = \"assets/data/\"\n",
    "filename = \"data\"\n",
    "pyspark_session = create_session()\n",
    "\n",
    "ailab_df = pyspark_session.read.parquet(RELATIVE_FOLDER_PATH +\"/data.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Preprocessing and Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T19:02:07.252596Z",
     "start_time": "2020-04-07T19:02:02.644982Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 66.4 ms, sys: 16.4 ms, total: 82.8 ms\n",
      "Wall time: 4.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ailab_df.cache().count()\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"stopWordsRemovedTokens\")\n",
    "hashingTF = HashingTF(inputCol=\"stopWordsRemovedTokens\", outputCol=\"rawFeatures\", numFeatures=2000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5)\n",
    "label_stringIdx = StringIndexer(inputCol = \"process_class\", outputCol = \"label\")\n",
    "\n",
    "pre_processing_pipeline = PySparkPipeline(stages=[tokenizer, remover, hashingTF, idf, label_stringIdx])\n",
    "\n",
    "pre_processing_pipeline_model = pre_processing_pipeline.fit(ailab_df)\n",
    "\n",
    "treated_df = pre_processing_pipeline_model.transform(ailab_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T19:02:17.497439Z",
     "start_time": "2020-04-07T19:02:07.254530Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.5 ms, sys: 0 ns, total: 13.5 ms\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kmeans = PySparkKMeans(k=20)\n",
    "kmeans_trained_model = kmeans.fit(treated_df)\n",
    "kmeans_result_df = kmeans_trained_model.transform(treated_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Classifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T19:02:30.681828Z",
     "start_time": "2020-04-07T19:02:17.506135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.5 ms, sys: 4.51 ms, total: 27 ms\n",
      "Wall time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "(trainingData, testData) = treated_df.randomSplit([0.7, 0.3], seed = 100)\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions_df = lrModel.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T19:03:07.416325Z",
     "start_time": "2020-04-07T19:02:30.686513Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.8 ms, sys: 437 µs, total: 11.2 ms\n",
      "Wall time: 36.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7279028242295433"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Storing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T19:10:58.440396Z",
     "start_time": "2020-04-07T19:10:19.382217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.45 ms, sys: 1.2 ms, total: 8.65 ms\n",
      "Wall time: 39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions.write.mode(\"overwrite\").format(\"parquet\").option(\"compression\", \"gzip\").mode(\"overwrite\").save(RELATIVE_FOLDER_PATH +\"pyspark_result.parquet\")\n",
    "kmeans_result_df.write.mode(\"overwrite\").format(\"parquet\").option(\"compression\", \"gzip\").mode(\"overwrite\").save(RELATIVE_FOLDER_PATH +\"pyspark_cluster_result.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark_session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Spacy e Sci-kit learn\n",
    "\n",
    "## 2.1 Loading files and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T19:12:21.661369Z",
     "start_time": "2020-04-07T19:12:18.263002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2036 entries, 0 to 2035\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   process_class  2036 non-null   object\n",
      " 1   process_id     2036 non-null   object\n",
      " 2   doc_id         2036 non-null   object\n",
      " 3   path_img       2036 non-null   object\n",
      " 4   text           2036 non-null   object\n",
      " 5   doc_type       2036 non-null   object\n",
      " 6   num_pag        2036 non-null   int64 \n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 111.5+ KB\n",
      "None\n",
      "CPU times: user 3.1 s, sys: 302 ms, total: 3.4 s\n",
      "Wall time: 3.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "VECTOR_MODEL_NAME = \"pt_core_news_sm\"\n",
    "NLP_SPACY = spacy.load(VECTOR_MODEL_NAME)\n",
    "RELATIVE_FOLDER_PATH = \"assets/data/\"\n",
    "filename = \"data\"\n",
    "stopwords_set = set(STOP_WORDS)\n",
    "\n",
    "parquet_filename = RELATIVE_FOLDER_PATH + filename + \".parquet.gzip\"\n",
    "ailab_df = pd.read_parquet(parquet_filename)\n",
    "print(ailab_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Preprocessing and Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T19:12:21.894443Z",
     "start_time": "2020-04-07T19:12:21.664025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 227 ms, sys: 205 µs, total: 227 ms\n",
      "Wall time: 226 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = NLP_SPACY.Defaults.create_tokenizer(NLP_SPACY)\n",
    "raw_text = ailab_df['text'].to_list()\n",
    "\n",
    "tokenized_text = []\n",
    "for row in raw_text[:20]:\n",
    "    doc = tokenizer(row)\n",
    "    preprocessed_doc = [token for token in doc if not token.norm_ in stopwords_set]\n",
    "    tokenized_text.append(\" \".join([word.text for word in preprocessed_doc]))\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "''' Encapsuling components in pipeline '''\n",
    "pipeline = Pipeline([\n",
    "    ('count_vectorizer', count_vectorizer),\n",
    "    ('tfidf_transformer', tfidf_transformer)\n",
    "])\n",
    "\n",
    "vectorized_docs = pipeline.fit_transform(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T19:14:41.603121Z",
     "start_time": "2020-04-07T19:14:41.149206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 953 ms, sys: 558 ms, total: 1.51 s\n",
      "Wall time: 443 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kmeans = KMeans(20)\n",
    "kmeans.fit(vectorized_docs)\n",
    "clustering = collections.defaultdict(list)\n",
    "\n",
    "for idx, label in enumerate(kmeans.labels_):\n",
    "    clustering[str(label)].append(str(idx))\n",
    "    \n",
    "kmeans_df = pd.DataFrame(clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Classyfing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T19:12:22.381084Z",
     "start_time": "2020-04-07T19:12:22.368994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.4 ms, sys: 7.72 ms, total: 18.1 ms\n",
      "Wall time: 6.48 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "targets_labels = ailab_df['process_class'].to_list()[:20]\n",
    "''' Let's evaluate more deeply the best model '''\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     vectorized_docs,\n",
    "    targets_labels,\n",
    "    test_size=0.25, random_state=42)\n",
    "\n",
    "clf = SGDClassifier()\n",
    "\n",
    "train1 = X_train\n",
    "labelsTrain1 = y_train\n",
    "test1 = X_test\n",
    "labelsTest1 = y_test\n",
    "\"\"\"  train \"\"\"\n",
    "clf.fit(train1, labelsTrain1)\n",
    "\"\"\"  test \"\"\"\n",
    "preds = clf.predict(test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T19:12:22.492198Z",
     "start_time": "2020-04-07T19:12:22.383916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 2, does not match size of target_names, 3. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/PythonEnviroments/ailab-researching/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   1993\u001b[0m                 \u001b[0;34m\"Number of classes, {0}, does not match size of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m                 \u001b[0;34m\"target_names, {1}. Try specifying the labels \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1995\u001b[0;31m                 \u001b[0;34m\"parameter\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1996\u001b[0m             )\n\u001b[1;32m   1997\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of classes, 2, does not match size of target_names, 3. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"accuracy:\", accuracy_score(labelsTest1, preds))\n",
    "print(\n",
    "    classification_report(\n",
    "        labelsTest1,\n",
    "        preds,\n",
    "        target_names=ailab_df['process_class'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Storing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T19:14:58.575542Z",
     "start_time": "2020-04-07T19:14:56.577066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.93 s, sys: 42.9 ms, total: 1.97 s\n",
      "Wall time: 1.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ailab_df['path_img'] = [ str(doc) for doc in ailab_df['path_img']]\n",
    "write(RELATIVE_FOLDER_PATH +\"result.parquet\", ailab_df, compression='gzip')\n",
    "write(RELATIVE_FOLDER_PATH +\"cluster_result.parquet\", kmeans_df, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
