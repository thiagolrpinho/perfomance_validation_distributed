{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T18:16:08.932354Z",
     "start_time": "2020-04-14T18:15:55.493700Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Modules used for PySpark solution\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer, StopWordsRemover, StringIndexer\n",
    "from pyspark.ml import Pipeline as PySparkPipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.clustering import KMeans as PySparkKMeans\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Modules used for non distributed solution\n",
    "import collections\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.cluster import KMeans\n",
    "from fastparquet import write \n",
    "import pandas as pd\n",
    "\n",
    "def create_session():\n",
    "    ''' Function used to instantiate a pySpark Session with \n",
    "    the specific configurations'''\n",
    "    sc_conf = SparkConf()\n",
    "    sc_conf.setAppName('SparkPreProcessing')\n",
    "    sc_conf.setMaster('local')\n",
    "    sc_conf.set('spark.executor.memory', '6g')\n",
    "    sc_conf.set('spark.executor.cores', '8')\n",
    "    sc_conf.set('spark.logConf', True)\n",
    "    print(sc_conf.getAll())\n",
    "    sc = SparkContext.getOrCreate(conf=sc_conf)\n",
    "    ss = SparkSession(sc)\n",
    "    return ss\n",
    "\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PySpark\n",
    "\n",
    "## 1.1 Loading Files and Creating Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T18:16:14.122401Z",
     "start_time": "2020-04-14T18:16:08.935946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.executor.memory', '6g'), ('spark.master', 'local'), ('spark.logConf', 'True'), ('spark.submit.deployMode', 'client'), ('spark.executor.cores', '8'), ('spark.ui.showConsoleProgress', 'true'), ('spark.app.name', 'SparkPreProcessing')]\n",
      "CPU times: user 20.3 ms, sys: 7.23 ms, total: 27.5 ms\n",
      "Wall time: 5.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Parquet files are self-describing so the schema is preserved.\n",
    "# The result of loading a parquet file is also a DataFrame.\n",
    "RELATIVE_FOLDER_PATH = \"../assets/data/\"\n",
    "filename = \"data\"\n",
    "pyspark_session = create_session()\n",
    "\n",
    "ailab_df = pyspark_session.read.parquet(RELATIVE_FOLDER_PATH +\"/data.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Preprocessing and Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T18:16:26.774025Z",
     "start_time": "2020-04-14T18:16:14.125098Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 75.9 ms, sys: 32 ms, total: 108 ms\n",
      "Wall time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ailab_df.cache().count()\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"stopWordsRemovedTokens\")\n",
    "hashingTF = HashingTF(inputCol=\"stopWordsRemovedTokens\", outputCol=\"rawFeatures\", numFeatures=2000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5)\n",
    "label_stringIdx = StringIndexer(inputCol = \"process_class\", outputCol = \"label\")\n",
    "\n",
    "pre_processing_pipeline = PySparkPipeline(stages=[tokenizer, remover, hashingTF, idf, label_stringIdx])\n",
    "\n",
    "pre_processing_pipeline_model = pre_processing_pipeline.fit(ailab_df)\n",
    "\n",
    "treated_df = pre_processing_pipeline_model.transform(ailab_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T20:56:13.294336Z",
     "start_time": "2020-04-07T20:56:01.699480Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.5 ms, sys: 1.83 ms, total: 18.4 ms\n",
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kmeans = PySparkKMeans(k=20)\n",
    "kmeans_trained_model = kmeans.fit(treated_df)\n",
    "kmeans_result_df = kmeans_trained_model.transform(treated_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Classifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T20:56:27.331347Z",
     "start_time": "2020-04-07T20:56:13.296535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26 ms, sys: 0 ns, total: 26 ms\n",
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "(trainingData, testData) = treated_df.randomSplit([0.7, 0.3], seed = 100)\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions_df = lrModel.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T20:57:03.736874Z",
     "start_time": "2020-04-07T20:56:27.333335Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.4 ms, sys: 0 ns, total: 10.4 ms\n",
      "Wall time: 36.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7459806286135295"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Storing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T20:57:40.130148Z",
     "start_time": "2020-04-07T20:57:03.738423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.66 ms, sys: 4.14 ms, total: 8.8 ms\n",
      "Wall time: 36.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions_df.write.mode(\"overwrite\").format(\"parquet\").option(\"compression\", \"gzip\").mode(\"overwrite\").save(RELATIVE_FOLDER_PATH +\"pyspark_result.parquet\")\n",
    "kmeans_result_df.write.mode(\"overwrite\").format(\"parquet\").option(\"compression\", \"gzip\").mode(\"overwrite\").save(RELATIVE_FOLDER_PATH +\"pyspark_cluster_result.parquet\")\n",
    "pyspark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T20:57:40.141501Z",
     "start_time": "2020-04-07T20:57:40.132902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark solution took: 0:01:50.810402\n"
     ]
    }
   ],
   "source": [
    "pyspark_time = datetime.now() - start_time\n",
    "print(\"PySpark solution took:\", pyspark_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T20:57:40.303405Z",
     "start_time": "2020-04-07T20:57:40.145371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark solution took: 0:01:50.975982\n"
     ]
    }
   ],
   "source": [
    "pyspark_time = datetime.now() - start_time\n",
    "print(\"PySpark solution took:\", pyspark_time)\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Spacy e Sci-kit learn\n",
    "\n",
    "## 2.1 Loading files and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T20:57:45.190975Z",
     "start_time": "2020-04-07T20:57:40.305053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2036 entries, 0 to 2035\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   process_class  2036 non-null   object\n",
      " 1   process_id     2036 non-null   object\n",
      " 2   doc_id         2036 non-null   object\n",
      " 3   path_img       2036 non-null   object\n",
      " 4   text           2036 non-null   object\n",
      " 5   doc_type       2036 non-null   object\n",
      " 6   num_pag        2036 non-null   int64 \n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 111.5+ KB\n",
      "None\n",
      "CPU times: user 4.46 s, sys: 334 ms, total: 4.79 s\n",
      "Wall time: 4.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "VECTOR_MODEL_NAME = \"pt_core_news_sm\"\n",
    "NLP_SPACY = spacy.load(VECTOR_MODEL_NAME)\n",
    "filename = \"data\"\n",
    "stopwords_set = set(STOP_WORDS)\n",
    "\n",
    "parquet_filename = RELATIVE_FOLDER_PATH + filename + \".parquet.gzip\"\n",
    "ailab_df = pd.read_parquet(parquet_filename)\n",
    "print(ailab_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Preprocessing and Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T20:58:34.926215Z",
     "start_time": "2020-04-07T20:57:45.192902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.6 s, sys: 106 ms, total: 49.7 s\n",
      "Wall time: 49.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = NLP_SPACY.Defaults.create_tokenizer(NLP_SPACY)\n",
    "raw_text = ailab_df['text'].to_list()\n",
    "\n",
    "tokenized_text = []\n",
    "for row in raw_text:\n",
    "    doc = tokenizer(row)\n",
    "    preprocessed_doc = [token for token in doc if not token.norm_ in stopwords_set]\n",
    "    tokenized_text.append(\" \".join([word.text for word in preprocessed_doc]))\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "''' Encapsuling components in pipeline '''\n",
    "pipeline = Pipeline([\n",
    "    ('count_vectorizer', count_vectorizer),\n",
    "    ('tfidf_transformer', tfidf_transformer)\n",
    "])\n",
    "\n",
    "vectorized_docs = pipeline.fit_transform(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T21:02:29.698763Z",
     "start_time": "2020-04-07T20:58:34.928236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 25s, sys: 43.2 s, total: 5min 8s\n",
      "Wall time: 3min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kmeans = KMeans(20)\n",
    "kmeans.fit(vectorized_docs)\n",
    "clustering = collections.defaultdict(list)\n",
    "kmeans_df = ailab_df.copy()\n",
    "kmeans_df['cluster_label'] = [label for label in kmeans.labels_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Classyfing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T21:02:30.023455Z",
     "start_time": "2020-04-07T21:02:29.701041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 675 ms, sys: 464 ms, total: 1.14 s\n",
      "Wall time: 317 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "targets_labels = ailab_df['process_class'].to_list()\n",
    "''' Let's evaluate more deeply the best model '''\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     vectorized_docs,\n",
    "    targets_labels,\n",
    "    test_size=0.25, random_state=42)\n",
    "\n",
    "clf = SGDClassifier()\n",
    "\n",
    "train1 = X_train\n",
    "labelsTrain1 = y_train\n",
    "test1 = X_test\n",
    "labelsTest1 = y_test\n",
    "\"\"\"  train \"\"\"\n",
    "clf.fit(train1, labelsTrain1)\n",
    "\"\"\"  test \"\"\"\n",
    "preds = clf.predict(test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T21:02:30.038269Z",
     "start_time": "2020-04-07T21:02:30.025360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8408644400785854\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          RE       0.00      0.00      0.00        10\n",
      "         ARE       0.84      0.87      0.86       274\n",
      "          AI       0.84      0.84      0.84       225\n",
      "\n",
      "    accuracy                           0.84       509\n",
      "   macro avg       0.56      0.57      0.57       509\n",
      "weighted avg       0.82      0.84      0.83       509\n",
      "\n",
      "CPU times: user 9.83 ms, sys: 0 ns, total: 9.83 ms\n",
      "Wall time: 8.94 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/PythonEnviroments/ailab-researching/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"accuracy:\", accuracy_score(labelsTest1, preds))\n",
    "print(\n",
    "    classification_report(\n",
    "        labelsTest1,\n",
    "        preds,\n",
    "        target_names=ailab_df['process_class'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Storing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T21:02:33.935317Z",
     "start_time": "2020-04-07T21:02:30.039863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.71 s, sys: 60.4 ms, total: 3.77 s\n",
      "Wall time: 3.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ailab_df['path_img'] = [ str(doc) for doc in ailab_df['path_img']]\n",
    "kmeans_df['path_img'] = [ str(doc) for doc in kmeans_df['path_img']]\n",
    "\n",
    "write(RELATIVE_FOLDER_PATH +\"result.parquet\", ailab_df, compression='gzip')\n",
    "write(RELATIVE_FOLDER_PATH +\"cluster_result.parquet\", kmeans_df, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T21:02:33.940528Z",
     "start_time": "2020-04-07T21:02:33.936933Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undistributed solution took: 0:04:53.636143\n"
     ]
    }
   ],
   "source": [
    "undistributed_time = datetime.now() - start_time\n",
    "print(\"Undistributed solution took:\", undistributed_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
